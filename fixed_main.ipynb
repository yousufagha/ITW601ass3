{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4ee62ba8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install selenium==4.20.0 pandas==2.2.2 webdriver-manager==4.0.1\n",
        "# !pip install selenium pandas webdriver-manager\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0d82d881",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Australian Entry-Level IT Jobs Scraper from Jora.com\n",
        "\n",
        "# Required Libraries\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from urllib.parse import urlparse, urlunparse\n",
        "import time\n",
        "import traceback\n",
        "\n",
        "# Function to clean URLs by removing query params\n",
        "def clean_url(url):\n",
        "    parsed = urlparse(url)\n",
        "    return urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))\n",
        "\n",
        "# Setup Selenium WebDriver (using Chrome)\n",
        "options = webdriver.ChromeOptions()\n",
        "driver = webdriver.Chrome(options=options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2cebb17f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Navigate to Jora.com\n",
        "driver.get('https://au.jora.com/')\n",
        "wait = WebDriverWait(driver, 20)\n",
        "\n",
        "# Input Job Search Query\n",
        "search_job = wait.until(EC.presence_of_element_located((By.XPATH, \"//input[@placeholder='Job title, company, keyword']\")))\n",
        "search_job.clear()\n",
        "search_job.send_keys('Entry Level IT')\n",
        "search_job.send_keys(Keys.RETURN)\n",
        "time.sleep(5)\n",
        "\n",
        "# Lists to store scraped data\n",
        "data = []\n",
        "visited_urls = set()\n",
        "visited_combinations = set()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e62ffc36",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No more pages or navigation issue.\n"
          ]
        }
      ],
      "source": [
        "# Extended list of 160+ common tech skills\n",
        "keyword_list = [\n",
        "    \"Python\", \"SQL\", \"Java\", \"JavaScript\", \"C++\", \"C#\", \"Ruby\", \"PHP\", \"Swift\", \"Kotlin\",\n",
        "    \"Go\", \"R\", \"MATLAB\", \"Perl\", \"Scala\", \"Shell\", \"Bash\", \"HTML\", \"CSS\", \"SASS\",\n",
        "    \"Django\", \"Flask\", \"Spring\", \"Node.js\", \"React\", \"Angular\", \"Vue.js\", \"Express.js\",\n",
        "    \"AWS\", \"Azure\", \"GCP\", \"Firebase\", \"Heroku\", \"Linux\", \"Windows Server\", \"Git\", \"GitHub\", \"Bitbucket\",\n",
        "    \"Jira\", \"Confluence\", \"Docker\", \"Kubernetes\", \"Terraform\", \"Ansible\", \"CI/CD\", \"Jenkins\",\n",
        "    \"Power BI\", \"Tableau\", \"Looker\", \"Excel\", \"Pandas\", \"NumPy\", \"TensorFlow\", \"PyTorch\",\n",
        "    \"NLP\", \"Machine Learning\", \"Deep Learning\", \"ETL\", \"BigQuery\", \"Snowflake\",\n",
        "    \"Splunk\", \"ServiceNow\", \"Selenium\", \"Appium\", \"QTP\", \"Postman\", \"Rest API\", \"SOAP\",\n",
        "    \"Agile\", \"Scrum\", \"Kanban\", \"DevOps\", \"ITIL\", \"VMware\", \"Citrix\", \"Azure DevOps\",\n",
        "    \"Salesforce\", \"Zoho\", \"HubSpot\", \"Dynamics 365\", \"Shopify\", \"WordPress\", \"WooCommerce\",\n",
        "    \"Magento\", \"Bootstrap\", \"Tailwind\", \"Webpack\", \"LESS\", \"GraphQL\", \"Redux\",\n",
        "    \"Linux Shell\", \"Zsh\", \"MySQL\", \"PostgreSQL\", \"MongoDB\", \"Oracle\", \"DB2\", \"MariaDB\",\n",
        "    \"Airflow\", \"Kafka\", \"Spark\", \"Hadoop\", \"Hive\", \"Cassandra\", \"Elasticsearch\", \"Redis\",\n",
        "    \"RESTful Services\", \"Microservices\", \"OAuth\", \"SAML\", \"SSO\", \"LDAP\", \"Bamboo\", \"Nagios\",\n",
        "    \"Prometheus\", \"Grafana\", \"New Relic\", \"Datadog\", \"Logstash\", \"Snort\", \"Wireshark\",\n",
        "    \"IT Support\", \"Service Desk\", \"Help Desk\", \"Desktop Support\", \"Active Directory\", \"Group Policy\",\n",
        "    \"Networking\", \"Switching\", \"Routing\", \"Firewalls\", \"VPN\", \"Remote Desktop\"\n",
        "]\n",
        "\n",
        "while len(data) < 200:\n",
        "    try:\n",
        "        job_cards = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[href*='/job/']\")))\n",
        "\n",
        "        for job in job_cards:\n",
        "            try:\n",
        "                url = job.get_attribute(\"href\")\n",
        "                cleaned_url = clean_url(url)\n",
        "                title = job.text.strip()\n",
        "\n",
        "                if cleaned_url in visited_urls or not cleaned_url:\n",
        "                    continue\n",
        "                visited_urls.add(cleaned_url)\n",
        "\n",
        "                driver.execute_script(\"window.open(arguments[0]);\", url)\n",
        "                driver.switch_to.window(driver.window_handles[1])\n",
        "                time.sleep(3)\n",
        "\n",
        "                if \"au.jora.com\" not in driver.current_url:\n",
        "                    driver.close()\n",
        "                    driver.switch_to.window(driver.window_handles[0])\n",
        "                    continue\n",
        "\n",
        "                body_text = driver.find_element(By.TAG_NAME, 'body').text\n",
        "                page_lines = body_text.split('\\n')\n",
        "\n",
        "                try:\n",
        "                    h1 = driver.find_element(By.TAG_NAME, 'h1')\n",
        "                    h1_parent = h1.find_element(By.XPATH, '..')\n",
        "                    company_block = h1_parent.text.split('\\n')\n",
        "                    company = next((line for line in company_block if line.strip() and line.strip() != title), \"N/A\")\n",
        "                except:\n",
        "                    company = \"N/A\"\n",
        "\n",
        "                if (title, company) in visited_combinations:\n",
        "                    driver.close()\n",
        "                    driver.switch_to.window(driver.window_handles[0])\n",
        "                    continue\n",
        "                visited_combinations.add((title, company))\n",
        "\n",
        "                try:\n",
        "                    location = next(line for line in page_lines if any(state in line for state in [\"NSW\", \"VIC\", \"QLD\", \"SA\", \"WA\", \"TAS\", \"ACT\", \"NT\"]))\n",
        "                except:\n",
        "                    location = \"N/A\"\n",
        "\n",
        "                try:\n",
        "                    date = next(line for line in page_lines if any(tag in line for tag in [\"ago\", \"Today\", \"day\"]))\n",
        "                except:\n",
        "                    date = \"N/A\"\n",
        "\n",
        "                try:\n",
        "                    marker_keywords = [\"Description\", \"Responsibilities\", \"What you\u2019ll do\", \"What You'll Do\", \"Your Role\"]\n",
        "                    desc_index = next((i for i, line in enumerate(page_lines) if any(marker in line for marker in marker_keywords)), 0)\n",
        "                    filtered_lines = [line for line in page_lines[desc_index:] if not any(boiler in line for boiler in ['Search jobs', 'Browse salaries', 'Find recruiters', 'Go to Employer site', 'Log in'])]\n",
        "                    description = \" \".join(filtered_lines).strip()\n",
        "                except:\n",
        "                    description = body_text.strip()\n",
        "\n",
        "                matched_skills = [skill for skill in keyword_list if skill.lower() in description.lower()]\n",
        "                skills_found = \", \".join(sorted(set(matched_skills))) if matched_skills else \"N/A\"\n",
        "\n",
        "                data.append({\n",
        "                    'Title': title,\n",
        "                    'Company': company,\n",
        "                    'Location': location,\n",
        "                    'Date Posted': date,\n",
        "                    'Description': description,\n",
        "                    'Skills': skills_found,\n",
        "                    'Job URL': cleaned_url\n",
        "                })\n",
        "\n",
        "                driver.close()\n",
        "                driver.switch_to.window(driver.window_handles[0])\n",
        "\n",
        "                if len(data) >= 200:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(\"Error scraping job details:\", e)\n",
        "                try:\n",
        "                    driver.close()\n",
        "                    driver.switch_to.window(driver.window_handles[0])\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        try:\n",
        "            next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'a[aria-label=\"Next\"]')))\n",
        "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
        "            time.sleep(3)\n",
        "        except:\n",
        "            print(\"No more pages or navigation issue.\")\n",
        "            break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Could not load job cards.\")\n",
        "        traceback.print_exc()\n",
        "        break\n",
        "# Closing the driver\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "11cd57b7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping complete. Found and saved 13 unique jobs to CSV file: australian_entry_level_it_jobs.csv\n"
          ]
        }
      ],
      "source": [
        "# Creating DataFrame and removing any rows with duplicate cleaned URLs\n",
        "df_jobs = pd.DataFrame(data)\n",
        "df_jobs = df_jobs.drop_duplicates(subset=['Job URL'], keep=False)\n",
        "df_jobs.to_csv('australian_entry_level_it_jobs.csv', index=False)\n",
        "print(f\"Scraping complete. Found and saved {len(df_jobs)} unique jobs to CSV file: australian_entry_level_it_jobs.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}